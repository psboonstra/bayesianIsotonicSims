---
title: "Section 3.2: Varying-data Evaluation (Run Sims)"
author: ""
date: ""
output: html_document
---

```{r setup, include = TRUE, echo = FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
klippy::klippy(position = c("top", "right"), tooltip_message = "Copy")
```

Running this script without modification produces an object called `arglist`,
which is a list of 8 lists, with each of the 8 lists corresponding to a unique
scenario, defined as a combinations of 2 sample sizes sizes and 4 probability
curves. Then, one of these lists is extracted based upon the value of
`array_id`, and the simulator function called simulator is called on this
scenario. Various results are written to files in the local folder called `out`.
This script expects that the following files exist:
`sim_functions/varying_data_generate_params.R` and
`sim_functions/varying_data_functions.R`.

### Setup

First install (if necessary) then load the required R packages.

```{r, message = F, warning = F}
library(tidyverse);
library(Iso);
library(cir);
library(binom);
library(isotonicBayes);
library(brms);
library(rstan);
```


```{r}
#Helpful to print warnings when they occur for debugging
options(warn = 1);
# Other options
rstan_options(auto_write = FALSE);
options(mc.cores = parallel::detectCores());
```

The code is written to be run on your personal computer (`my_computer = T`, 
for purposes of development and testing) or a high-performance cluster using 
the SLURM scheduler (`my_computer = F`, for actually running the entire study). 
The simulation study was conducted in three separate batches. Vary `which_batch`
to indicate this.

```{r}
my_computer = FALSE  
which_batch = 1 # Choose from 1, 2, 3, 4
```

There are two versions of the HSIPV prior implemented and five versions of the GAIPV
prior implemented. The variable names correspond to the methods as follows:

  - `horseshoe1` = HSIPV$(0.001)$
  - `horseshoe2` = HSIPV$(10)$
  - `gamma1` = GAIPV$(0.5/(K+1), \epsilon)$
  - `gamma2` = GAIPV$(0.5/(K+1), \epsilon/10)$
  - `gamma3` = GAIPV$(0.5/(K+1), \epsilon/100)$
  - `gamma4` = GAIPV$(0.5/(K+1))$
  - `gamma5` = GAIPV$(1)$

In addition, standard isotonic regression and the BRMS monotone method are
implemented.

The varying-data evaluation in Section 3.3 was conducted in four batches of 400
jobs each. The batches differ in the methods that they implement and the
data-generating curves that they use: (i) batches 1 and 3 (`which_batch = 1`;
`which_batch = 3`) implement the fast methods (`horseshoe1`, `horseshoe2`,
`gamma1`, and `gamma5`). You will need to request a short time allocation
(conservatively ~1 hour) on the cluster for these batches. (ii) batches 2 and 4
(`which_batch = 2`; `which_batch = 4`) implement the slow methods (`gamma2`,
`gamma3`, `gamma4`). You will need to request a long time allocation
(conservatively ~3 days) on the cluster for these batches. (iii) batches 1 and 2
implement the original two data generating curves crossed with two sample sizes
(n = 80; n = 320) (iv) batches 3 and 4 implement two other data generating
curves (based upon reviewer comments) crossed with two sample sizes (n = 80; n =
320)

Since each batch contains 400 jobs and covers 4 data-generating scenarios (two
curves crossed with two sample sizes), each batch dedicates 100 jobs to a
data-generating scenario (`jobs_per_scenario = 100`). Further, each job creates
two independent datasets (`n_sim=2`), and so in total there will be 200 datasets
generated and analyzed for every data-generating scenario. The same seeds are
used across batches, and so batches 1 and 2, which differ only in the methods
used, will generate identical datasets. Similarly, batches 3 and 4 will also
generate identical datasets.

The `array_id` variable is key: on your local computer you must choose its value;
on SLURM, it is an environmental variable that is accessed. 

```{r}
jobs_per_scenario = 100;
n_sim = 2;
if(my_computer) {
  #Choose from between 1-400 if varying_data_generate_params.R is used as-is
  array_id = 2;
} else {
  array_id = as.numeric(Sys.getenv('SLURM_ARRAY_TASK_ID'));
}
```

`permute_array_ids` controls whether the sim_id labels should be randomly
permuted across array_ids. Do this if you plan to check the results along the
way and want to ensure a good representation of all scenarios. If `FALSE`, then
all jobs from the same scenario will occur in contiguous blocks. Regardless,
this will not affect your results once all simulations have run to completion.

```{r}
permute_array_ids = TRUE
```

`array_id_offset` is added to the label of the saved object. It is required to
keep the labels distinct across multiple batches.

```{r}
array_id_offset = (which_batch - 1) * (jobs_per_scenario * 4);

if(which_batch %in% c(1,3)) {#first/third batches
  
  # Request 59 minutes running time for this batch (using 2 nodes)

  # This is the main simulation study;
  do_hs = TRUE;
  hs_scales = c(
    horseshoe1 = 0.001,
    horseshoe2 = 10)
  hs_prior_types = c(
    horseshoe1 = "horseshoe",
    horseshoe2 = "horseshoe")
  hs_slab_precisions = c(
    horseshoe1 = 1,
    horseshoe2 = 1)
  do_ga = TRUE;
  ga_shapes = c(gamma1 = quote(0.5 / length_unique_x),
                gamma5 = 1)
  ga_lower_bounds = c(gamma1 = .Machine$double.eps,
                      gamma5 = 0);
  
  include_comparators = TRUE;
  
} else if(which_batch %in% c(2,4)) {#second/fourth batches
  
  # Request 2 days running time for this batch (using 1 node)
  
  # This batch studies the effect of pushing the gamma lower bound closer to zero
  do_hs = FALSE
  do_ga = TRUE;
  ga_shapes = c(gamma2 = quote(0.5 / length_unique_x),
                gamma3 = quote(0.5 / length_unique_x),
                gamma4 = quote(0.5 / length_unique_x))
  ga_lower_bounds = c(gamma2 = 10^(log10(.Machine$double.eps) - 1), 
                      gamma3 = 10^(log10(.Machine$double.eps) - 2), 
                      gamma4 = 0);
  include_comparators = FALSE;
  
} else {
  stop("'which_batch' must be in 1,2,3,4")
}

if(which_batch %in% c(1,2)) {#first/second batches
  true_prob_curve_list = list(
    # Curve 1: simple linear increase
    `1` = function(x) {pmin(1, pmax(0, x));},
    # Curve 2: Two sharp increases
    `2` = function(x) {
      0.35 * plogis(x, location = 0.25, scale = 0.015) +
        0.65 * plogis(x, location = 0.75, scale = 0.015);
    }
  )
} else {#third/fourth batches
  true_prob_curve_list = list(
    # Curve 3: One sharp increase
    `3` = function(x) {
      .3 * plogis(x, location = 0.25, scale = 0.015);
    },
    # Curve 4: Smooth, positive first and second derivatives
    `4` = function(x) {pmin(1, pmax(0, x^4));}
  )
} 
```

Before calling the next lines, you should specify any parameters that 
you wish to change from their default values. Any specified values will
take precedence over the default values that will otherwise be set by
sourcing `varying_data_generate_params.R`.

```{r}
source("sim_functions/varying_data_generate_params.R");
source("sim_functions/varying_data_functions.R");
```

Here we permute the array ids if requested:
```{r}
if(permute_array_ids) {
  permute_array_ids = seq(1, jobs_per_scenario * length(arglist), by = jobs_per_scenario)
  set.seed(2);
  permute_array_ids = 
    c(permute_array_ids, 
      setdiff(sample(jobs_per_scenario * length(arglist)), permute_array_ids));
  
} else {
  permute_array_ids = 1:(jobs_per_scenario * length(arglist));
}
```

Now we are ready to extract the specific list of arguments of interest, as 
determined by the value of `array_id`. 

```{r}
# It's wasteful, but we only need the single id for this job
curr_args = arglist[[ceiling(permute_array_ids[array_id]/jobs_per_scenario)]];
# Ensure that the datasets are identical within array_ids and different 
# between array_ids
curr_args[["random_seed"]] = array_id;

# This is the actual call to the simulator function
assign(paste0("job",array_id_offset + array_id),
       do.call("simulator",args = curr_args));

# Save the entire workspace in case you want to dig in
do.call("save",list(paste0("job",array_id_offset + array_id),
                    file = paste0("out/job",array_id_offset + array_id,".RData"),
                    precheck = FALSE));
# Also just save the performance as a csv 
write_csv(get(paste0("job",array_id_offset + array_id))$summarized_performance,
          path = paste0("out/job",array_id_offset + array_id,"_performance.csv"),
          append = FALSE);

write_csv(get(paste0("job",array_id_offset + array_id))$summarized_bayesian_performance,
          path = paste0("out/job",array_id_offset + array_id,"_bayesian_performance.csv"),
          append = FALSE);

if(curr_args[["return_summarized_models"]]) {
  write_csv(get(paste0("job",array_id_offset + array_id))$summarized_models,
            path = paste0("out/job",array_id_offset + array_id,"_models.csv"),
            append = FALSE);
}

```
